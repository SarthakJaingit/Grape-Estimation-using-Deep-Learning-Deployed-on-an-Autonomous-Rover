# -*- coding: utf-8 -*-
"""GrapeYieldSegmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17QhmTPbtDMjVRMXBdC0HyQB5jOrbR_G8
"""

import glob
import os 
import cv2
import numpy as np
import matplotlib.pyplot as plt
import json
import torch 
from torchvision.transforms import functional as F

!git clone https://github.com/thsant/wgisd.git

class GrapeDataset():
  def __init__(self, data_root):

    self.mask_npz = sorted(glob.glob(os.path.join(data_root, "*.npz")))
    images_jpg = sorted(glob.glob(os.path.join(data_root, "*.jpg")))
    bbox_txt = sorted(glob.glob(os.path.join(data_root, "*.txt")))
    assert len(images_jpg) == len(bbox_txt)

    # In wgisd dataset not all masks are paired with bbox and imgs. 
    image_names = [os.path.splitext(os.path.basename(fp))[0] for fp in images_jpg]
    mask_names = [os.path.splitext(os.path.basename(fp))[0] for fp in self.mask_npz]
    removable_ind = [ii for ii, n in enumerate(image_names) if n not in mask_names]

    self.images_jpg, self.bbox_txt = list(), list()
    for ii in range(len(images_jpg)):
      if ii not in removable_ind:
        self.images_jpg.append(images_jpg[ii])
        self.bbox_txt.append(bbox_txt[ii])

    # Checks on dataset
    assert len(self.images_jpg) == len(self.mask_npz) == len(self.bbox_txt)
    for ii in range(len(self.images_jpg)):
      assert os.path.splitext(os.path.basename(self.images_jpg[ii]))[0] == \
      os.path.splitext(os.path.basename(self.bbox_txt[ii]))[0] == \
      os.path.splitext(os.path.basename(self.mask_npz[ii]))[0]
    
    print("Dataset Passed Assertions")


  def __getitem__(self, idx):

    # Handle the img
    img = cv2.imread(self.images_jpg[idx])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = F.to_tensor(img)

    # Handle the mask
    masks = np.load(self.mask_npz[idx])["arr_0"].astype(np.uint8)
    masks = np.moveaxis(masks, -1, 0)
    num_objs = masks.shape[0]
    masks = torch.as_tensor(masks, dtype=torch.uint8)

    # Handle bboxes
    np_bbox_text = np.loadtxt(self.bbox_txt[idx], delimiter = " ", dtype = np.float32)
    bboxes = np_bbox_text[:, 1:]
    
    assert (bboxes.shape[0] == num_objs)

    _, height, width = img.shape
    scaled_boxes = []
    for box in bboxes:
        x1 = box[0] - box[2]/2
        x2 = box[0] + box[2]/2
        y1 = box[1] - box[3]/2
        y2 = box[1] + box[3]/2
        scaled_boxes.append([x1 * width, y1 * height, x2 * width, y2 * height])
    
    scaled_boxes = torch.as_tensor(scaled_boxes, dtype=torch.float32)

    #Create labels from masks
    labels = torch.ones((num_objs,), dtype=torch.int64)
    image_id = torch.tensor([idx])

    target = {
            "boxes": scaled_boxes,
            "labels": labels,
            "masks": masks,
            "image_id": image_id
        }
    
    return img, target

  def __len__(self):
    return len(self.images_jpg)

train_batch_size = 2
def collate_fn(batch):
  return tuple([list(a) for a in zip(*batch)])

grape_Dataset = GrapeDataset("/content/wgisd/data")
train_loader = torch.utils.data.DataLoader(grape_Dataset, batch_size = train_batch_size, shuffle = False, collate_fn = collate_fn)

def draw_boxes(boxes, image):
    # read the image with OpenCV
    image = image.permute(1, 2, 0).numpy()
    for i, box in enumerate(boxes):
      cv2.rectangle(
          image,
          (int(box[0]), int(box[1])),
          (int(box[2]), int(box[3])),
          (0, 0, 255), 2
      )
    
    return image

dataiter = iter(train_loader)
images, targets = next(dataiter)

fig = plt.figure(figsize=(25, 4))
for idx in np.arange(2):
    ax = fig.add_subplot(2, 2/2, idx+1, xticks=[], yticks=[])
    image = draw_boxes(targets[idx]["boxes"], images[idx])
    plt.imshow(image)